{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle pandas\n",
        "!pip install opendatasets\n",
        "!pip install wget\n",
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "mDYPdxPrYfP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGIBgLZVeCY4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.typing import NDArray\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "import spacy\n",
        "from scipy.sparse import issparse\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from typing import Text,List,Iterator,Iterable,Union, Tuple\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "import string\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from itertools import count\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "#from gensim import corpora\n",
        "from sklearn.metrics import classification_report\n",
        "import random\n",
        "import opendatasets as od\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading data and checking the data balance and NULL values"
      ],
      "metadata": {
        "id": "at7izUGp5wM7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "Y-DvIocAeSrX"
      },
      "outputs": [],
      "source": [
        "#Reading data\n",
        "\"\"\" This is a publicly available fake and real news dataset on Kaggle. To download the dataset two options are commented on here.\n",
        "In here, I have used the open datasets(od) to download the data from the Kaggle.\n",
        " However, the second option might also work if anyone chooses to apply.\n",
        " As a disclaimer I would like to explicitly state that I have no explicit license over this data, and it is only intended for educational purposes not for any distribution.\n",
        "\"\"\"\n",
        "#od.download(\"https://www.kaggle.com/datasets/nopdev/real-and-fake-news-dataset\")\n",
        "file_path = \"/content/real-and-fake-news-dataset/news.csv\"\n",
        "#df = kagglehub.load_dataset(KaggleDatasetAdapter.PANDAS,\"nopdev/real-and-fake-news-dataset\",path=file_path)\n",
        "\n",
        "#df = (\"https://www.kaggle.com/datasets/nopdev/real-and-fake-news-dataset\")\n",
        "data = pd.read_csv(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGalOCZSfjrq"
      },
      "outputs": [],
      "source": [
        "#visualizing the data\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8r9w3zGkfxFJ"
      },
      "outputs": [],
      "source": [
        "#Checking the data values\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOBnAuzVgIfE"
      },
      "outputs": [],
      "source": [
        "#Checking the balance of real and fake news data using a bar chart\n",
        "data[\"label\"].value_counts().plot(kind=\"bar\",figsize=(5,8),color=[\"blue\",\"black\"],title=\"posts related to fake news\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "POStags and NERtags Processing and visualizations"
      ],
      "metadata": {
        "id": "qV5-SULR6McD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Run the code sequentially to avoid unnecessary errors as the previous assists the next flow.However,fake_spacydocs and\n",
        "real_spacydocs takes a while probably more than 5 minutes. Hence, avoid repeated running of this section of the code.\"\"\""
      ],
      "metadata": {
        "id": "P7O-n64a3VMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "kwDrd2RX5AqT"
      },
      "outputs": [],
      "source": [
        "#initalize spacymodel\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "qz8MZBrg6Pti"
      },
      "outputs": [],
      "source": [
        "#Spliting the data in to fake and real news\n",
        "Fake_news = data[data[\"label\"] == \"FAKE\"]\n",
        "Real_news = data[data[\"label\"] == \"REAL\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "Wg1lN5FP7sDE"
      },
      "outputs": [],
      "source": [
        "#Creating spacy documents  for each group of news\n",
        "fake_spacydocs = list(nlp.pipe(Fake_news[\"text\"]))\n",
        "real_spacydocs = list(nlp.pipe(Real_news[\"text\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting Fake news and Real news Postags and NER tags"
      ],
      "metadata": {
        "id": "jgZxzaxZ4yl8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "XqhrA8cX8hRj"
      },
      "outputs": [],
      "source": [
        "#A function to extract POStags and NER tags from each of the rows of the document using spacy\n",
        "def extract_token_tags(doc:spacy.tokens.doc.Doc) -> List[Tuple[Text,Text]]:\n",
        "  return [(token.text,token.tag_,token.pos_) for token in doc]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting Fake news POStags and NER tags"
      ],
      "metadata": {
        "id": "JJKZA1zw5mdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fakenews_tags_process(fake_spacydocs):\n",
        "\n",
        "     fake_tagsdf = []\n",
        "     columns = [\"Token\",\"NER_tag\",\"POS_tag\"]\n",
        "     for i, doc in enumerate(fake_spacydocs):\n",
        "         tags = extract_token_tags(doc)\n",
        "         tags = pd.DataFrame(tags,columns=columns)\n",
        "         fake_tagsdf.append(tags)\n",
        "     ###Concatenating the tags in the panda dataframe to the right format\n",
        "     fake_tagssdf = pd.concat(fake_tagsdf)\n",
        "     #Frequency of fake news POS_tags\n",
        "     Fake_postag_counts =  fake_tagssdf.groupby([\"Token\", \"POS_tag\"]).size().reset_index(name=\"count\").sort_values(by=\"count\",ascending=False)[:20]\n",
        "     display(Fake_postag_counts.head())\n",
        "     #Frequency of fake news entities\n",
        "     Fake_entities=  fake_tagssdf.groupby([\"Token\", \"NER_tag\"]).size().reset_index(name=\"count\").sort_values(by=\"count\",ascending=False)[:20]\n",
        "     display(Fake_entities.head())\n",
        "     #Graphical visualizations of Fake news NER_tags\n",
        "     purple_palette = ['#7c3f00','#633200','#562b00']\n",
        "     sns.barplot(\n",
        "       x = \"count\",\n",
        "       y = \"Token\",\n",
        "       hue = \"NER_tag\",\n",
        "       data = Fake_entities,\n",
        "       palette=purple_palette,\n",
        "       dodge = False\n",
        "      ).set(title = \"Most common Named Entities in Fake News data\")\n",
        "     plt.show()\n",
        "    #calling the function for visualization\n",
        "fakenews_tags_process(fake_spacydocs)"
      ],
      "metadata": {
        "id": "yOKuB_P7QhWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting Real news Postags and NER tags"
      ],
      "metadata": {
        "id": "2AYGlilsszWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def realnews_tags_process(real_spacydocs):\n",
        "   columns = [\"Token\",\"NER_tag\",\"POS_tag\"]\n",
        "   real_tagsdf = []\n",
        "\n",
        "   for i, doc in enumerate(real_spacydocs):\n",
        "      tags = extract_token_tags(doc)\n",
        "      tags = pd.DataFrame(tags,columns=columns)\n",
        "      real_tagsdf.append(tags)\n",
        "   #Concatenating the tags in the panda dataframe to the right format\n",
        "   real_tagssdf = pd.concat(real_tagsdf)\n",
        "   #Frequencies of real news POS_tags\n",
        "   Real_postag_counts = real_tagssdf.groupby([\"Token\", \"POS_tag\"]).size().reset_index(name=\"count\").sort_values(by=\"count\",ascending=False)[:20]\n",
        "   display(Real_postag_counts.head())\n",
        "   #Frequency of real news entities\n",
        "   Real_entities = real_tagssdf.groupby([\"Token\", \"NER_tag\"]).size().reset_index(name=\"count\").sort_values(by=\"count\",ascending=False)[:20]\n",
        "   display(Real_entities.head())\n",
        "   #Graphical visualizations of Real news NER_tags\n",
        "   purple_palette = ['#9F2B68', '#BF40BF', '#CF9FFF']\n",
        "   sns.barplot(\n",
        "       x = \"count\",\n",
        "       y = \"Token\",\n",
        "       hue = \"NER_tag\",\n",
        "       data = Real_entities,\n",
        "       palette=purple_palette,\n",
        "       dodge = False\n",
        "   ).set(title = \"Most common Named Entities in Real News data\")\n",
        "   plt.show()\n",
        "   #calling the function for visualization\n",
        "realnews_tags_process(real_spacydocs)"
      ],
      "metadata": {
        "id": "wgOs3VrGjV1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text preprocessing"
      ],
      "metadata": {
        "id": "IMHFJjTsUP4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"This preprocess_data function plays major role in preprocessing the data extracted. It includes a helper text_clean function\n",
        " that is a template function that tells how each row in the text column of the data be cleaned.\n",
        " In this helper function regular expression, wordnet lemmatizer and english stop words were used to clean text.Then this text cleaning function is applied to the data.\"\"\""
      ],
      "metadata": {
        "id": "5b2o7d8ND6Ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data: pd.DataFrame,text_column: str):\n",
        "    en_stopwords = stopwords.words(\"english\")\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    #data[\"cleaned_text\"] = data.apply(lambda row_txt: re.sub(r\"^[^-]*[^-]*-\\s*\", \"\", str(row_txt[\"text\"])), axis= 1)\n",
        "    def text_clean(text):\n",
        "\n",
        "       #Removing consequetive hyphens and white spaces from the start of a token\n",
        "       text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "       text = re.sub(r\"^[^-]*[^-]*-\\s*\", \"\", text)\n",
        "       #Removing leading white spaces\n",
        "       text = re.sub(r\"(^\\s+|\\s+$)\", \" \", text)\n",
        "       #Lower casing characters with regular expression\n",
        "       text = text.lower()\n",
        "       #Removing spaces, punctuation, digits, etc... using regular expression\n",
        "       text = re.sub(r\"([^\\w\\s])\", \" \",text)\n",
        "       text = re.sub(r\"([.,;!?])\", \" \", text)\n",
        "       text = re.sub(r\"(\\d+)\", \" \", text)\n",
        "       text = re.sub(r\"(\\n{2,})\", \" \",text)\n",
        "       text = re.sub(r\"(\\s{2,})\", \" \",text)\n",
        "       #Tokenizing the cleaned text\n",
        "       Tokens = word_tokenize(text)\n",
        "       #Removing stop words from the cleaned documents\n",
        "       text = [Token for Token in Tokens if Token not in (en_stopwords)]\n",
        "       #Applying lemmatization on the cleaned text\n",
        "       Lemmatized_text = ' '.join(lemmatizer.lemmatize(Token) for Token in Tokens)\n",
        "       return Lemmatized_text\n",
        "    cleaned_texts = data[\"text\"].apply(text_clean)\n",
        "    data[\"cleaned_text\"] = cleaned_texts\n",
        "    return data[\"cleaned_text\"]"
      ],
      "metadata": {
        "id": "rSkpHr4PX70I"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment Analysis using vader sentiment analyzer which is mostly applied to social media data sentiment analysis"
      ],
      "metadata": {
        "id": "qDS76-odXfev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"This sentiment analysis class was required mainly to separate the data input of this sentiment anlayzer from the classifier.\n",
        "The sentiment analyzer accepts uncleaned data.Beacuse the purpose of this class preparation is mainly for isolation purpose\n",
        "bins and labels are set at the inializations, it is not possible to pass the bin values while running unless the user sets\n",
        "another bin values or edit the class code.\"\"\""
      ],
      "metadata": {
        "id": "EPa9kjBV9cZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentAnalyzer:\n",
        "    def __init__(self, data: pd.DataFrame):\n",
        "        \"\"\"Initializes the SentimentAnalyzer with a DataFrame.\"\"\"\n",
        "        self.data = data\n",
        "        self.sentiment_results = None\n",
        "        self.custom_plot_colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "        self.bins = [-1, -0.1,0.1, 1]\n",
        "        self.labels = [\"Negative\", \"Neutral\", \"Positive\"]\n",
        "        self.vader_sentiment = SentimentIntensityAnalyzer()    # Instantiate VADER\n",
        "    def analyze_sentiment(self):\n",
        "        \"\"\"Analyzes sentiment of the text data.\"\"\"\n",
        "        self.data[\"sentiment_score\"] = self.data[\"text\"].map(lambda x: self.vader_sentiment.polarity_scores(x)[\"compound\"])\n",
        "        self.data.loc[:,\"vader_sentiment_label\"] = pd.cut(self.data[\"sentiment_score\"], bins=self.bins, labels=self.labels)\n",
        "        self.sentiment_results = self.data[['text', 'sentiment_score', 'vader_sentiment_label']]\n",
        "        self.data.head()\n",
        "        return self.sentiment_results\n",
        "    def sentiment_distribution_visualizer(self):\n",
        "        self.custom_plot_colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "        sns.countplot(x=\"vader_sentiment_label\", data=self.data, palette=self.custom_plot_colors).set(title=\"Sentiment Distribution\")\n",
        "        plt.show()\n",
        "    def sentiment_valuecount_visualizer(self):\n",
        "        self.custom_plot_colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "        self.data['vader_sentiment_label'].value_counts().plot(kind='bar', color=self.custom_plot_colors)\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "gg1iJx09k5mh"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text classifier using TFIDF vectorizer and Logistic regression"
      ],
      "metadata": {
        "id": "HPshZZ5xmvd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"This text classifier takes cleaned text from the preprocess_data function cleaned using regular expression and lemmatized using lematizer.\n",
        "Hence,before running this class make sure to run the preprocess_data function above.\"\"\""
      ],
      "metadata": {
        "id": "ogkQzmUbA-Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassifier:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initializes the TextClassifier with a cleaned text DataFrame and Label column.\"\"\"\n",
        "        self.le = LabelEncoder()\n",
        "        self.vectorizer = TfidfVectorizer(ngram_range=(1, 3),max_df=0.9,min_df=5,\n",
        "        stop_words=None,\n",
        "        sublinear_tf=True)\n",
        "        self.model = LogisticRegression(solver='liblinear',class_weight='balanced',random_state=42)\n",
        "        self.y = [] # label column from the data input\n",
        "        self.X = [] # cleaned text\n",
        "    def train_classifier(self, data: pd.DataFrame, label: str):\n",
        "        \"\"\"Trains a classifier on the cleaned text data.\"\"\"\n",
        "        self.X = data\n",
        "        self.y = label\n",
        "        self.y = self.le.fit_transform(self.y)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=42)\n",
        "        Xj_train = [' '.join(tokens) for tokens in X_train]\n",
        "        Xj_test = [' '.join(tokens) for tokens in X_test]\n",
        "        X_tvec = self.vectorizer.fit_transform(X_train)\n",
        "        X_ttest = self.vectorizer.transform(X_test)\n",
        "        self.model.fit(X_tvec, y_train)\n",
        "        # predict based on the segregated test data\n",
        "        y_pred = self.model.predict(X_ttest)\n",
        "        assert y_test.shape == y_pred.shape\n",
        "        #accuracy_score from the model\n",
        "        accuracy = self.model.score(X_ttest, y_test)\n",
        "        F1_score = f1_score(y_test, y_pred)\n",
        "        print(f'Model accuracy: {accuracy:.2f}')\n",
        "        print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Cz8hwV7qQ9qV"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Incase if you experience any variations in classification results it might be helpful to apply what is commented here.\n",
        "However, for this dataset it is unlikely for that to happen.\"\"\"\n",
        "#random.seed(0)\n",
        "#np.random.seed(0)"
      ],
      "metadata": {
        "id": "zqqjJpciDezM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying the sentiment analyzer and Text classifier on the fake news data"
      ],
      "metadata": {
        "id": "tZhn08EjIhkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    #Reading data\n",
        "    # This is a publicly available fake and real news dataset on Kaggle\n",
        "    file_path = \"/content/real-and-fake-news-dataset/news.csv\"\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # This is the label column labeled as label in the data file.\n",
        "    label_column = data[\"label\"]\n",
        "    # Sentiment Analysis\n",
        "    sentiment_analyzer = SentimentAnalyzer(data)\n",
        "\n",
        "    # Analyze entire DataFrame\n",
        "    sentiment_results = sentiment_analyzer.analyze_sentiment()\n",
        "    print(\"Sentiment Analysis Results\")\n",
        "    print(sentiment_results)\n",
        "\n",
        "    # Plot sentiment value counts and distribution\n",
        "    sentiment_analyzer.sentiment_valuecount_visualizer()\n",
        "    sentiment_analyzer.sentiment_distribution_visualizer()\n",
        "     # Preprocessing the text from the data\n",
        "    cleaned_text_col = preprocess_data(data,\"text\")\n",
        "\n",
        "     ###Classifier\n",
        "    fakenews_classifier = TextClassifier()\n",
        "    fakenews_classifier.train_classifier(cleaned_text_col,label_column)"
      ],
      "metadata": {
        "id": "fkKZPwxKy_Nh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}